{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 03:16:40.320820: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-20 03:16:42.262902: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-20 03:16:42.272356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-20 03:16:46.636915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (ds_train, ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 03:33:46.383428: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_2aa66\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2aa66_level0_col0\" class=\"col_heading level0 col0\" >label</th>\n",
       "      <th id=\"T_2aa66_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2aa66_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2aa66_row0_col0\" class=\"data row0 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_2aa66_row0_col1\" class=\"data row0 col1\" >There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel&#x27;s absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven&#x27;t laughed this hard since I saw THE FULL MONTY. (And, even then, I don&#x27;t think I laughed quite this hard... So to speak.) Tukel&#x27;s talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there&#x27;s none of the over-the-top scenery chewing one might&#x27;ve expected from a film like this). DING-A-LING-LESS is a film whose time has come.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2aa66_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2aa66_row1_col0\" class=\"data row1 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_2aa66_row1_col1\" class=\"data row1 col1\" >A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel&#x27;s previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.&lt;br /&gt;&lt;br /&gt;The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual&#x27;s struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin&#x27;s whole existence and reason for being seems to be to help others, whether [...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2aa66_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2aa66_row2_col0\" class=\"data row2 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_2aa66_row2_col1\" class=\"data row2 col1\" >Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making &quot;Superhero Movie&quot; the eleventh in a series that single handily ruined the parody genre. Now I&#x27;ll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you&#x27;ve milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.&lt;br /&gt;&lt;br /&gt;You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he&#x27;s not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 [...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2aa66_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2aa66_row3_col0\" class=\"data row3 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_2aa66_row3_col1\" class=\"data row3 col1\" >Poor Shirley MacLaine tries hard to lend some gravitas to this mawkish, gag-inducing &quot;feel-good&quot; movie, but she&#x27;s trampled by the run-away sentimentality of a film that&#x27;s not the least bit grounded in reality.&lt;br /&gt;&lt;br /&gt;This was directed by Curtis Hanson? Did he have a lobotomy since we last heard from him? Hanson can do effective drama sprinkled with comedy, as evidenced by &quot;Wonder Boys.&quot; So I don&#x27;t know what happened to him here. This is the kind of movie that doesn&#x27;t want to accept that life is messy and fussy, and that neat, tidy endings (however implausible they might be) might make for a nice closing shot, but come across as utterly phony if the people watching the film have been through anything remotely like what the characters in the film go through.&lt;br /&gt;&lt;br /&gt;My wife and I made a game of calling out the plot points before they occurred -- e.g. &quot;the old man&#x27;s going to teach her to read and then [...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  b\"There are films that make careers. For Georg...\n",
       "1      1  b\"A blackly comic tale of a down-trodden pries...\n",
       "2      0  b'Scary Movie 1-4, Epic Movie, Date Movie, Mee...\n",
       "3      0  b'Poor Shirley MacLaine tries hard to lend som..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert a tensor flow dataset to a dataframe\n",
    "x = tfds.as_dataframe(ds_test.take(4), ds_info)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer._check_trainability of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f3d94692a90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: [Errno 9] Bad file descriptor\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer._check_trainability of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f3d94692a90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: [Errno 9] Bad file descriptor\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method KerasLayer._check_trainability of <tensorflow_hub.keras_layer.KerasLayer object at 0x7f3d94692a90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: [Errno 9] Bad file descriptor\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)\n",
    "\n",
    "# Batch and prefetch the datasets for performance\n",
    "batch_size = 32\n",
    "ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)# why ?\n",
    "\n",
    "# Load the preprocessor and ALBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 8140s 6s/step - loss: 0.7121 - accuracy: 0.4816 - val_loss: 0.7074 - val_accuracy: 0.4754\n",
      "Epoch 2/3\n",
      " 354/1250 [=======>......................] - ETA: 1:09:34 - loss: 0.7028 - accuracy: 0.4963"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)\n",
    "\n",
    "# Reduce batch size to lessen memory usage\n",
    "batch_size = 16\n",
    "ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Load the preprocessor and ALBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Enable mixed precision (if GPU supports it)\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "set_global_policy(policy)\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 8155s 7s/step - loss: 0.6989 - accuracy: 0.5001 - val_loss: 0.6972 - val_accuracy: 0.5040\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 8309s 7s/step - loss: 0.6937 - accuracy: 0.5191 - val_loss: 0.6891 - val_accuracy: 0.5424\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 8894s 7s/step - loss: 0.6887 - accuracy: 0.5398 - val_loss: 0.6845 - val_accuracy: 0.5582\n",
      "1563/1563 [==============================] - 8053s 5s/step - loss: 0.6840 - accuracy: 0.5554\n",
      "Test Accuracy: 0.5554\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)\n",
    "\n",
    "# Reduce batch size to lessen memory usage\n",
    "batch_size = 16\n",
    "ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Load the preprocessor and ALBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/albert_en_base/3\")\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#### SMALLER BERT MODEL#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 143s 109ms/step - loss: 0.7052 - accuracy: 0.5330 - val_loss: 0.6974 - val_accuracy: 0.5468\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 135s 108ms/step - loss: 0.7009 - accuracy: 0.5345 - val_loss: 0.7033 - val_accuracy: 0.5282\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 136s 108ms/step - loss: 0.6970 - accuracy: 0.5397 - val_loss: 0.6937 - val_accuracy: 0.5498\n",
      "1563/1563 [==============================] - 137s 87ms/step - loss: 0.6955 - accuracy: 0.5432\n",
      "Test Accuracy: 0.5432\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)\n",
    "\n",
    "# Reduce batch size to lessen memory usage\n",
    "batch_size = 16\n",
    "ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Load the preprocessor and DistilBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\")\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 03:39:14.977090: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_644ce\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_644ce_level0_col0\" class=\"col_heading level0 col0\" >label</th>\n",
       "      <th id=\"T_644ce_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_644ce_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_644ce_row0_col0\" class=\"data row0 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_644ce_row0_col1\" class=\"data row0 col1\" >This was an absolutely terrible movie. Don&#x27;t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie&#x27;s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor&#x27;s like Christopher Walken&#x27;s good name. I could barely sit through it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_644ce_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_644ce_row1_col0\" class=\"data row1 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_644ce_row1_col1\" class=\"data row1 col1\" >I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_644ce_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_644ce_row2_col0\" class=\"data row2 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_644ce_row2_col1\" class=\"data row2 col1\" >Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. &lt;br /&gt;&lt;br /&gt;But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? &lt;br /&gt;&lt;br /&gt;Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.&lt;br /&gt;&lt;br /&gt;Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_644ce_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_644ce_row3_col0\" class=\"data row3 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_644ce_row3_col1\" class=\"data row3 col1\" >This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  b\"This was an absolutely terrible movie. Don't...\n",
       "1      0  b'I have been known to fall asleep during film...\n",
       "2      0  b'Mann photographs the Alberta Rocky Mountains...\n",
       "3      1  b'This is the kind of film for a snowy Sunday ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tfds.as_dataframe(ds_train.take(4), ds_info)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_val = train_val_split(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size to lessen memory usage\n",
    "batch_size = 16\n",
    "ds_train = ds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessor and DistilBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 177s 136ms/step - loss: 1.1266 - accuracy: 0.4983 - val_loss: 0.8518 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 197s 157ms/step - loss: 0.7511 - accuracy: 0.5056 - val_loss: 0.7078 - val_accuracy: 0.4996\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 192s 152ms/step - loss: 0.7029 - accuracy: 0.5070 - val_loss: 0.6974 - val_accuracy: 0.5140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb4c6743ac0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 171s 109ms/step - loss: 0.6955 - accuracy: 0.5234\n",
      "Test Accuracy: 0.5234\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple of pre and postprocessed_text\n",
    "\n",
    "for text, label in ds_train.take(5):\n",
    "    preprocessed_text = preprocessor(text)\n",
    "    print(f'Original Text: {text.numpy()}')\n",
    "    print(f'Preprocessed Text: {preprocessed_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.5225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 01:22:18.794044: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 937635840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 165s 131ms/step - loss: 0.6954 - accuracy: 0.5225 - val_loss: 0.6917 - val_accuracy: 0.5350\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.5343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 01:25:37.970866: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 937635840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 159s 126ms/step - loss: 0.6917 - accuracy: 0.5343 - val_loss: 0.6879 - val_accuracy: 0.5428\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.5456"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 01:28:20.978113: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 937635840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 162s 129ms/step - loss: 0.6872 - accuracy: 0.5456 - val_loss: 0.6843 - val_accuracy: 0.5530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb4c5b46e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 148s 94ms/step - loss: 0.6829 - accuracy: 0.5604\n",
      "Test Accuracy: 0.5604\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard in the notebook\n",
    " %load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Change with a another model to see if the accuracy improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessor and DistilBERT model from TensorFlow Hub\n",
    "#preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "#encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\")\n",
    "\n",
    "# choose a model from TensorFlow Hub\n",
    "# use base BERT model\n",
    "# preprocessor_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "# encoder_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\"\n",
    "\n",
    "\n",
    "# use base ALBERT model\n",
    "preprocessor_url =\"https://tfhub.dev/tensorflow/albert_en_preprocess/3\"\n",
    "encoder_url =\"https://tfhub.dev/tensorflow/albert_en_base/3\"\n",
    "\n",
    "# load preprocesor and encodor\n",
    "preprocessor = hub.KerasLayer(preprocessor_url)\n",
    "encoder = hub.KerasLayer(encoder_url)\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping, tensorboard_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SST-2 (Stanford Sentiment Treebank) for sentiment analysis\n",
    "\n",
    "Binary classification: positive/negative sentiment labels\n",
    "Sentences from movie reviews\n",
    "11,855 instances\n",
    "Fully labeled parse trees for compositional sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset without `as_supervised=True`\n",
    "dataset, info = tfds.load('glue/sst2', with_info=True, batch_size=-1, as_supervised=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataset info\n",
    "print(\"Dataset Info:\")\n",
    "print(info)\n",
    "\n",
    "# Print specific details about the dataset\n",
    "print(\"\\nFeatures:\")\n",
    "print(info.features)\n",
    "\n",
    "print(\"\\nSplits:\")\n",
    "for split, split_info in info.splits.items():\n",
    "    print(f\"{split}: {split_info}\")\n",
    "\n",
    "print(\"\\nDescription:\")\n",
    "print(info.description)\n",
    "\n",
    "print(\"\\nHomepage:\")\n",
    "print(info.homepage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and labels\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "\n",
    "# Extract texts and labels from the dataset\n",
    "train_texts = train_data['sentence'].numpy()\n",
    "train_labels = train_data['label'].numpy()\n",
    "val_texts = val_data['sentence'].numpy()\n",
    "val_labels = val_data['label'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts type: <class 'numpy.ndarray'>\n",
      "Train labels type: <class 'numpy.ndarray'>\n",
      "Val texts type: <class 'numpy.ndarray'>\n",
      "Val labels type: <class 'numpy.ndarray'>\n",
      "Train texts shape: (67349,)\n",
      "Train labels shape: (67349,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train texts type:\", type(train_texts))\n",
    "print(\"Train labels type:\", type(train_labels))\n",
    "print(\"Val texts type:\", type(val_texts))\n",
    "print(\"Val labels type:\", type(val_labels))\n",
    "print(\"Train texts shape:\", train_texts.shape)\n",
    "print(\"Train labels shape:\", train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching at fit step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 04:06:40.817285: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c1a28\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c1a28_level0_col0\" class=\"col_heading level0 col0\" >label</th>\n",
       "      <th id=\"T_c1a28_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c1a28_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c1a28_row0_col0\" class=\"data row0 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_c1a28_row0_col1\" class=\"data row0 col1\" >Australia&#x27;s first mainstream slasher film hits the screen with a bang. And a stab. And a slice. And a scream or two. And plenty of blood, frights, red herrings and lots of laughs.&lt;br /&gt;&lt;br /&gt;In fact, there&#x27;s lots of first surrounding Cut - it&#x27;s the first script of Dave Warner&#x27;s to be produced, although he has several others either optioned or in negotiation; it&#x27;s the first major film from director and former Hoodoo Guru Kimble Rendall; and it&#x27;s also the first film for producer Martin Fabinyi. And for a bunch of guys dipping their toes into this genre for the first time, they sure know their stuff.&lt;br /&gt;&lt;br /&gt;Cut tells the story of a bunch of Australian film students who hear about a slasher film, Hot Blooded, that was never finished because its director, Hilary (Kylie Minogue), was killed by the actor playing the psycho killer in the film.&lt;br /&gt;&lt;br /&gt;Despite their lecturer (who was assistant director on the night [...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1a28_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c1a28_row1_col0\" class=\"data row1 col0\" >0 (neg)</td>\n",
       "      <td id=\"T_c1a28_row1_col1\" class=\"data row1 col1\" >Yes, In 35 years of film going I have finally viewed the stinker that surpasses all other ghastly movies I have seen. Beating &#x27;Good Will Hunting&#x27; Baise Moi&#x27; and &#x27;Flirt&#x27; for sheer awfulness. This is pretentious blige of the first order... not even entertaining pretentious bilge. The effects are cheap, and worse - pointless.&lt;br /&gt;&lt;br /&gt;The script seems to have been written by a first year film student who doesn&#x27;t get out much but wants to appear full of portent! The acting is simply undescribably bad - Tilda Swinton caps a career filled with vacuous woodeness with a performance which veers neurotically between comotose and laughable &#x27;intensity&#x27;. Apparently, some fool out there has allowed the director of this film to make another one... be warned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1a28_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c1a28_row2_col0\" class=\"data row2 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_c1a28_row2_col1\" class=\"data row2 col1\" >I&#x27;m a pretty old dude, old enough to remember the taste of Oreos and Coke as they were 50-55 years ago, when every taste for a kid was fresh. I wish I have somehow set some aside then is some magical suspended locker, so that I could taste those things today. This magical locker might even have adjusted the fabric of the food to account for how I&#x27;ve drifted, physically and otherwise, a sort of dynamic chemistry of expectations. Over the half century, they would have had to adjust quite a bit, because you see I would have known that I set them aside. Eating one now would be a celebration of self and past, and story, and sense that would almost make the intervening years an anticipated reward.&lt;br /&gt;&lt;br /&gt;I didn&#x27;t have enough sense to do that with original Coke. And I couldn&#x27;t have invented one of those magical psychic lockers Â— not then. But I did something almost as good. In the seventies, I really tuned into Roman Polanski. He was a strange and [...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1a28_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c1a28_row3_col0\" class=\"data row3 col0\" >1 (pos)</td>\n",
       "      <td id=\"T_c1a28_row3_col1\" class=\"data row3 col1\" >Miriam Hopkins is &quot;The Lady with Red Hair&quot; in this 1940 biopic of Mrs. Leslie Carter which also stars Claude Rains as David Belasco, Richard Ainley as Lou Payne, and a fine cast of supporting players, including Laura Hope Crews and Victor Jory.&lt;br /&gt;&lt;br /&gt;Miriam Hopkins and Claude Rains give wonderful performances. Hopkins was a beautiful actress who really makes us feel for Mrs. Carter. Rains is great as the flamboyant, egotistical producer/writer/actor/impresario David Belasco, one of the great names in theater.&lt;br /&gt;&lt;br /&gt;Though Mrs. Carter&#x27;s second husband, Lou Payne, served as adviser on this film, it&#x27;s a poor representation of the real events of Mrs. Carter&#x27;s life. True, there was a much publicized and bitter divorce, and she was undoubtedly viewed as a scandalous character for that and for becoming an actress. However, she had custody of her son Dudley, so there was no custody battle. Once she broke with Belasco, she did not [...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  b\"Australia's first mainstream slasher film hi...\n",
       "1      0  b\"Yes, In 35 years of film going I have finall...\n",
       "2      1  b'I\\'m a pretty old dude, old enough to rememb...\n",
       "3      1  b'Miriam Hopkins is \"The Lady with Red Hair\" i..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0= tfds.as_dataframe(ds_train.take(4), ds_info)\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tfds.as_dataframe(ds_train.take(4), ds_info)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessor and DistilBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'keras_layer_2' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=() dtype=string>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer 'keras_layer_2' (type KerasLayer):\n      â€¢ inputs=tf.Tensor(shape=(), dtype=string)\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Batch size provided directly in fit function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedqia791d.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m     result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope))), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     73\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_6\u001b[39m():\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (result,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmart_cond\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope))), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'keras_layer_2' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=() dtype=string>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer 'keras_layer_2' (type KerasLayer):\n      â€¢ inputs=tf.Tensor(shape=(), dtype=string)\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "# Batch size provided directly in fit function\n",
    "batch_size = 32\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping], batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'keras_layer' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=() dtype=string>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer 'keras_layer' (type KerasLayer):\n      â€¢ inputs=tf.Tensor(shape=(), dtype=string)\n      â€¢ training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Batch size provided directly in fit function\u001b[39;00m\n\u001b[1;32m     33\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(ds_test\u001b[38;5;241m.\u001b[39mbatch(batch_size))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filedqia791d.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m     result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope))), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     73\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_6\u001b[39m():\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (result,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmart_cond\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file5xm6a9iw.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope))), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact((\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope)))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n    File \"/tmp/__autograph_generated_file5xm6a9iw.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope))), ag__.autograph_artifact((lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope)))), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'keras_layer' (type KerasLayer).\n    \n    in user code:\n    \n        File \"/home/gilles/.pyenv/versions/3.8.12/envs/nlp05env/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 250, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=() dtype=string>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer 'keras_layer' (type KerasLayer):\n      â€¢ inputs=tf.Tensor(shape=(), dtype=string)\n      â€¢ training=True\n"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset using TensorFlow Datasets\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "def train_val_split(ds, val_size=0.2):\n",
    "    ds = ds.shuffle(10000, seed=42)\n",
    "    val_size = int(val_size * ds_info.splits['train'].num_examples)\n",
    "    ds_val = ds.take(val_size)\n",
    "    ds_train = ds.skip(val_size)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "ds_train, ds_val = train_val_split(ds_train)\n",
    "\n",
    "# Load the preprocessor and DistilBERT model from TensorFlow Hub\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2\")\n",
    "\n",
    "# Build the model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = preprocessor(text_input)\n",
    "encoder_outputs = encoder(preprocessed_text)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(pooled_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with batching done inside the `fit` function\n",
    "epochs = 3\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Batch size provided directly in fit function\n",
    "batch_size = 16\n",
    "model.fit(ds_train, validation_data=ds_val, epochs=epochs, callbacks=[early_stopping], batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(ds_test.batch(batch_size))\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp05env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
